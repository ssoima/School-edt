\documentclass{scrartcl}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bigdelim}
\usepackage{color}
\usepackage{ulem}
\usepackage{amssymb}
\usepackage{framed}
\usepackage{scrpage2} 

\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
%Aufgabenenvironment
\newenvironment{kapitel}[1][]{
\vspace{0.3cm}  
\textbf{Kapitel: #1} \\
\vspace{0.3cm} 
\begin{itemize}}{\end{itemize}}


\pagestyle{scrheadings} 
\clearscrheadfoot 
\setheadsepline{1.1pt}
\ihead[]{} 
\ohead[]{DWT - Zusammenfassung - \today} 
\cfoot[\pagemark]{\pagemark}
\begin{document}
\begin{flushleft}
\begin{kapitel}[Diskrete Wahrscheinlichkeit]
	\item Boolsche Ungleichung: \\
			\[ Pr \left[\bigcup\limits_{i=1}^{n} A_i \right] \leq \sum\limits_{i=1}^{n} Pr(A_i) \]
	\item Bedingte Wahrscheinlichkeit
		\begin{itemize}
				\item \[ Pr[A \vert B] := \frac{Pr[A \cap B]}{Pr[B]} \]
				\item \[ Pr[A \cap B] = Pr[A \vert B] \cdot Pr[A] = Pr[A \vert B] \cdot Pr[B]\]
				\item Seien die Ereignisse $A_1, ..., A_n$ gegeben. Falls $Pr[A_1 \cap ... \cap A_n]$ ist, gilt
							\begin{align}
							Pr[A_1 \cap ... \cap A_n] &= \\
							Pr[A_1] \cdot Pr[A_2\vert A_1] \cdot Pr[A_3\vert A_1 \cap A_2] \cdot ... \\
							& ... \cdot Pr[A_n \vert A_1 \cap ... \cap A_{n-1}]
							\end{align}
		\end{itemize}
	\item Satz der totalen Wahrscheinlichkeit: \\
			Die Ereignisse $A_1, ..., A_n$ seien paarweise disjunkt und es gelte: $B \subseteq A_1 \cup ... \cup A_n$ \\
			$\Rightarrow$ 
			\[ Pr[B] = \sum\limits_{i=1}^{n} Pr[B \vert A_i] \cdot Pr[A_i] \]
	\item Die Ereignisse A und B heissen unabhängig, wenn gilt: $PR[A \cap B] = Pr[A] \cdot Pr[B]$
	\item Satz von Bayes: \\
				Die Ereignisse $A_1, ..., A_n$ seien paarweise disjunkt, mit $Pr[A_j] > 0$ für alle j. Ferner sei 
				$B \subseteq A_1 \cup \cdots \cup A_n$ ein Ereignis mit $Pr[B] > 0$. Dann gilt für ein beliebiges $i = 1, ..., n$ \\
				\[ Pr[A_i \vert B] = \frac{Pr[A_i \cap B}{Pr[B]} = \frac{Pr[B \vert A_i] \cdot Pr[A_i]}{\sum_{j=1}^{n} Pr[B \vert A_j] \cdot Pr[A_j]} \]
				Gilt auch für Summen bis $\infty$
	\item $X: \Omega \rightarrow \mathbb{R}$ heißt Zufallsvariable
	\item (diskrete) Dichte(funktion) der Zufallsvariablen X: \\
				$f_X: \mathbb{R} \ni x \mapsto Pr[X=x] \in [0,1]$
	\item Verteilungsfunktion der Zufallsvariable X: \\
				$F_X: \mathbb{R} \ni x \mapsto Pr[X \leq x] = \sum\limits_{x \in W_x: x' \leq x} Pr[X = x'] \in [0,1]$
	\item \textbf{Erwartungswert} $\mathbb{E}[X]$ \\
				\[ \mathbb{E}[X] := \sum\limits_{x \in W_X} x \cdot Pr[X=x] = \sum\limits_{x \in W_X} x \cdot 			  
				f_{X}(x)\] sofern $\sum\limits_{X \in W_X} \vert x \vert \cdot Pr[X=x]$ konvergiert
	\item Berechnung des Erwarungswerts?? \\
				$\mathbb{E}[X] = \sum\limits_{x \in W_X} x \cdot Pr[X = x] = $ \\
						$\sum\limits_{x \in W_X} x \sum\limits_{\omega \in \Omega; X(\omega) = x} Pr[\omega] = $
						$= \sum\limits_{\omega \in \Omega} X(\omega) \cdot Pr[\omega]$ \\
						Achtung: Konvergenz bei unendlichen Wahrscheinlichkeitsräumen
	\item Monotonie des Erwartungswerts \\
				Seien X und Y Zufallsvariablen über dem Wahrscheinlichkeitsraum $\Omega$ mit $X(\omega) \leq Y(\omega)$
				für alle $\omega \in \Omega$. Dann gilt $\mathbb{E}[X] \leq \mathbb{E}[y]$ \\
				D.h. auch: Wenn $a \leq X(\omega) \leq b$ für alle $\omega \in \Omega$ erfüllt ist, dann gilt: \\
					$a \leq \mathbb{E}[X] \leq b$
	\item Wenn X eine Zufallsvariable ist, dann ist auch $f(X)$ eine Zufallsvariable
	\item \textbf{Linearität des Erwarungswertes} \\
				Für eine Beliebige Zufallsvariable X und $a,b \in \mathbb{R}$ gilt 
				\[ \mathbb{E}[a \cdot x + b] = a \cdot \mathbb{E}[X] + b \]
	\item Sei X eine Zufallsvariable und A ein Ereignis mit $Pr[A] > 0$. Die bedingte Zufallsvariable 
				$X \vert A$ besitzt die Dichte: \\
				$f_{X \vert A}(x) := Pr[X=x \vert A] = \frac{Pr["X = x" \cap A]}{Pr[A]}$ \\
				Erwartungswert berechnet sich durch: 
				$\mathbb{E}[X \vert A] = \sum\limits_{x \in W_X} x \cdot f_{X \vert A}(x)$
	\item X = Zufallsvariable: Für paarweise disjunkte Ereignisse 
		$A_1,..., A_n$ mit $A_1 \cup ...\cup A_n = \Omega$ und $Pr[A_1], ..., Pr[A_n] > 0$ gilt : \\
			\[ \mathbb{E}[X] = \sum\limits_{i=1}^{n} \mathbb{E}[X \vert A_i] \cdot Pr[A_i] \]
			analog für unendliche Reige, solang sie konvergiert
	\item \textbf{Varianz}: \\
				Var[X] := $\mathbb{E}[(X - \mu)^2] = \sum\limits_{x \in W_X} (x - \mu)^2 \cdot Pr[X = x]$ \\
				$\sigma := \sqrt{Var[X]}$ heißt \textbf{Standardabweichung} von X
	\item Für eine beliebige Zufallsvariable X gilt: \\
				$Var[X] = \mathbb{E}[X^2] - \mathbb{E}[X]^2$
	\item Für eine bliebige Zufallsvariable X und $a,b \in \R$ gilt: \\
				$Var[a \cdot X + b] = a^2 \cdot Var[X]$ \\
				Wichtig!!: $Var[X + b] = Var[X]$
	\item Für Zufallsvariable X nennen wir $\E[X^k]$ das k-te Moment und $\E[(X - \E[X])^k]$ das k-te
				zentrale Moment. Erwatungswer = erstes Moment, Variant = zweites zentrales Moment
	\item \[ f_{X,Y}(x,y) := Pr[X=x, Y=y] \]
				heißt gemeinsame Dichte der Zufallsvariablen X und Y 
				\[ f_X(x) = \sum\limits_{y \in W_Y} f_{X,Y}(x,y) \text{ bzw } 
						f_Y(y) = \sum\limits_{x \in W_X} f_{X,Y}(x,y)\]
						sind Randdichten
	\item $F_{X,Y}(x,y) = Pr[X \leq x, Y \leq Y] = Pr[\{\omega; X(\omega) \leq x, Y(\omega) \leq y \}] = 
				\sum\limits_{x' \leq x} \sum\limits_{y' \leq y} f_{X,Y} (x', y')$
				heißt gemeisame Verteilung
	\item Randverteilung + Randdichte?
	\item Die Zufallsvariablen $X_1, ..., X_n$ heißen unabhängig, wenn für alle $(x_1,...,x_n) \in 
						W_{X_1} \times ... \times W_{X_n}$ gilt: \\
						$Pr[X_1 = x_1, ..., X_n = x_n] = Pr[X_1 = x_1] \cdot .... \cdot Pr[X_n = x_n]$
	\item Wenn Zufallsvariablen unabhängig sind, dann gilt das auch noch, wenn man auf alle irgendwelche
				Funlktionen anwendet
	\item \textbf{[]} Für zwei \textsl{unabhängige} Zufallsvariablen X und Y sei $Z := X + Y$ \\
				Es gilt: \[ f_Z(z) = \sum\limits_{x \in W_X} f_X(X) \cdot f_Y(z-x) \]
	\item \textbf{[]} Linearität des Erwartungswetes: \\
				Für Zufallsvariablen $X_1, ..., X_n$ und $X := a_1X_1 + \cdot + a_nX_n$ mit 
				$a_1, ..., a_n \in \mathbb{R}$ gilt
				\[ \E[X] = a_1\E[X_1] + \cdots + a_n \E[X_n] \]
	\item Multiplikativität des Erwatungwertes: \\
				Für \textbf{unabhängige} Zufallsvariablen $X_1, ..., X_n$ gilt \\
					$\E[X_1 \cdot ... \cdot X_n] = \E[X_1] \cdot ... \cdot \E[X_n]$
	\item Additivität der Varianz:
				Für \textbf{unabhängige} Zufallsvariablen $X_1, ..., X_n$ und $X : = X_1 + ... + X_n$ gilt:  
				\[ Var[X] = Var[X_1] + ... + Var[X_n] \]
	\item Indikatoravariable = 1, wenn Ereigniss eintrott, 0, wenn nicht.
	\item Verteilungen: 
			\begin{itemize}
					\item Bernoulliverteilung: \\
								Zufallsvariable X mit $W_X = \{0,1\}$ \\
								$f_X(x) = \begin{cases} p & \text{ für } x = 1 \\ 
																				1-p & \text{ für } x = 0\end{cases}$ \\
								$q = 1 - p$ \\
								$\E[X] = p$ und $Var[X] = pq$	
					\item Binomialverteilung: \\
								$X := X_1 + ... + X_n$, Summe von n Bernoulliverteilten Indikatorvariablen. \\
								$\Rightarrow X \sim Bin(n,p)$ \\
								$f_X(x) := b(x;n,p) = {n \choose x} p^x q^{n-x}$ \\
								$\E[X] = n \cdot p$  $Var[X] = n \cdot p \cdot q$
					\item Warten auf den n-ten Erfolg (einschließlich) (negative Binomialverteilung mit Ordnung n): \\
								n unabhängige Zufallsvariablen $X_1, ..., X_n$: \\
								\[ f_Z(z) = { z-1 \choose n-1 } \cdot p^n \cdot (1-p)^{z-n}\]
					\item Wenn $X \sim Bin(n_x, p)$ und $Y \sim Bin(n_y, p)$ unabhängig, dann gilt für $Z := X + Y$,
								dass $Z \sim Bin(n_x + n_y, p)$
					\item geometrische Verteilung: \\
								$f_X(i) = p \cdot q^{i-1}$ für $i \in \mathbb{N}$ \\
								$\E[X] = \frac{1}{p}$ und $Var[X] = \frac{q}{p^2}$
					\item Poisson-Verteilung: \\
								Parameter $\lambda$ = Erwartungswert und auch Varianz\\
								Dichte: $f_X(i) = \frac{e^{- \lambda} \cdot \lambda^i}{i!}$ für $i \in \mathbb{N}_0$
								$X \sim Po(\lambda)$
								Poisson für hinreichend große n als approximation der Binomialverteilung verwendbar \\
								(Gesetz seltener Ereignisse, da Wkeit von einzelnem Treffer 
								recht klein sein muss, dass poisson geht) \\
								Voraussetzungen für Poisson: \\
								\begin{itemize}
								\item 	Ereignisse treten nie zur Gleichen Zeit auf  \\
								\item 	Wkeit, dass Ereignis in Zeitintervall auftritt ist 
												proportional zur länge des Intervalls \\
								\item		Anzahl der Ereignisse in festem Zeitintervall 
												hängt nur von dessem Länge ab, aber nicht von der Länge aufder Zeitachse 
								\item 	Wenn man zwei Disjunkte Zeitintervalle Betrachtet, 
												so sind die Anzahlen der Ereignisse in den Zeiträumen voneinander unabhängig.
								\end{itemize}
								Summe von Poissonverteilten Zufallsvariablen: \\
								X und Y unabhängig, $X \sim Po(\lambda)$ und $Y \sim Po(\mu)$, dann gilt: 
								\[ Z := X + Y \sim Po(\lambda + \mu) \]
			\end{itemize}
			\item Herausfinden ob Dichte zulässig: Summe nach Unendlich bilden und schauen ob 1 rauskommt
			\item \textbf{Markov Ungleichung} \\
						X ist Zufallsvariable, die \textbf{nur nichtnegative} Werte annimmt.
						$\Rightarrow \forall t \in \R$ mit $t > 0$:
						\[ Pr[X \geq t] \leq \frac{\E[X]}{t} \]
						bzw. Äquivalent
						\[ Pr[X \geq t \cdot \E[X]] \leq \frac{1}{t} \] 
			\item Chebyshev-Ungleichung \\
						Sei X eine Zufallsvariable und $t \in \R$ mit $t > 0$, dann gilt: 
						\[ Pr[\vert X - \E[X] \vert \geq t] \leq \frac{Var[X]}{t^2} \]
						bzw. Äquivalent
						\[ Pr[\vert X - \E[X] \vert \geq t \sqrt{Var[X]}] \leq \frac{1}{t^2} \]
			\item Gesetz der großen Zahlen: \\
						Geg: Zufallsvariablen X, $\epsilon, \delta > 0$ beliebig aber fest. \\
						Dann gilt für alle $n \geq \frac{Var[X]}{\epsilon \delta^2}$: \\
						Sind $X_1, ..., X_n$ unabhängige Zufallsvariablen mit der selben Verteilung wie X
						und setzt man
						\[ Z := \frac{X_1 + \cdots + X_n}{n} \]
						dann gilt: $Pr[\vert Z - \E[X] \vert \geq \delta] \leq \epsilon$ \\
						Wichtig: Gesetz der großen Zahlen schätzt die relative Abweichung \\
						\[ \vert \frac{1}{n} \sum_i X_i - p \]
						und nicht die absolute Abweichung 
						\[ \vert \sum_i X_i - np \]
						ab.
			\item Chernoff-Schranken \\
						$X_1, ..., X_n$ unabhängig, Bernoulliverteilt mit $Pr[X_i = 1] = p_i$ und
						$Pr[X_i = 0] = 1 - p_i$. Dann gilt für $X := \sum_{i=1}^{n}$ und 
						$\mu := \E[X] = \sum_{i=1}^{n} p_i$ sowie jedes $\delta > 0$, dass \\
						\[Pr[X \geq (1+ \delta) \mu] \leq \left( \frac{e^{\delta}}{(1+\delta)^{1+\delta}}\right)^{\mu}\]
						Beispiel: $\frac{n}{2} \cdot (1 + 10\%)$ \\
						$\Rightarrow$ Abweichung $= 0,1$, Erwartungswert $= n$ \\
						\[ \left( \frac{e^{0,1}}{(1+0,1)^{1+0,1}}\right)^{n} \]
			\item Seien $X_1, ..., X_n$ unabhängige Bernoulli-verteilte Zufallsvariablen mit $Pr[X_i = 1] = p_i $
						und $Pr[X_i = 0] = 1 - p_i$. Dann gilt für $X := \sum_{i=1}^{n} X_i$ und 
						$\mu := \E[X] = \sum_{i=1}^{n} p_i$ so wie jedes $0 < \delta < 1$ (hier ist der Unterschied zu 
						Chernoff), dass 
						\[ Pr[X \leq (1-\delta)\mu] \leq \left( \frac{e^{-\delta}}{(1-\delta)^{1-\delta}} \right)^{\mu} \]
			\item Für $o \leq \delta < 1$ gilt: 
						\[ (1-\delta)^{1-\delta} \geq e^{-\delta+\delta^2 / 2} \text{ und } 
								(1+ \delta)^{1 + \delta} \geq e^{\delta + \delta^2 / 3} \]
			\item Seien $X_1, ..., X_n$ unabhängige Bernoulli-verteilte Zufallsvariablen mit $Pr[X_i = 1] = p_1$
						und $Pr[X_i = 0] = 1 - p_i$. Dann gelten folgende Ungleichungen für $X := \sum_{i=1}^{n} X_i$
						und $\mu := \E[X] = \sum_{i=1}^{n} p_i$:
						\begin{itemize}
							\item $Pr[X \geq (1+\delta)\mu] \leq e^{\mu\delta^2 / 3}$ für alle $0 < \delta \leq 1,81$
							\item $Pr[X \geq (1-\delta)\mu] \leq e^{\mu\delta^2 / 2}$ für alle $0 < \delta \leq 1$
							\item $Pr[\vert X - \mu\vert \geq \delta\mu] \leq 2e^{-\mu\delta^2/3}$
											für alle $0 < \delta \leq 1$
							\item $Pr[X \geq (1+\delta)\mu] \leq \left( \frac{e}{1+\delta} \right)^{(1+\delta)\mu}$
							\item $Pr[X \geq t] \leq 2^{-t}$ für $t \geq 2e\mu$
						\end{itemize}
\end{kapitel}



\end{flushleft}
\end{document}
